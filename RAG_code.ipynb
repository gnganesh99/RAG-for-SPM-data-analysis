{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8b13da",
   "metadata": {},
   "source": [
    "# RAG for STM data analysis\n",
    "\n",
    "Here we developed a RAG agent to interface with custom data analysis code. \n",
    "\n",
    "The user interacts with the agent to analyse STM data. The agent then provides new code that can be used for plotting data.\n",
    "\n",
    "We use the \"test_rag_output.ipynb\" to the RAG prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fdc46",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f563a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ggn\\1_py_scripts\\RAG\\chat_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718af2a6",
   "metadata": {},
   "source": [
    "## Function for persist_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def get_persist_dir(\n",
    "    base_dir: str | Path,\n",
    "    basename: str,\n",
    "    *,\n",
    "    new_persist: bool = False,\n",
    "    create: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Find (or create) a persist directory under base_dir.\n",
    "\n",
    "    Looks for directories named: f\"{basename}_{run_id}\" where run_id is an int.\n",
    "    - If new_persist=False: returns the latest existing folder if found, else basename_0\n",
    "    - If new_persist=True : returns a new folder with run_id = (latest + 1) or 0 if none exist\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir).expanduser().resolve()\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pattern = re.compile(rf\"^{re.escape(basename)}_(\\d+)$\")\n",
    "\n",
    "    max_run_id: Optional[int] = None\n",
    "    for p in base_path.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        run_id = int(m.group(1))\n",
    "        if max_run_id is None or run_id > max_run_id:\n",
    "            max_run_id = run_id\n",
    "\n",
    "    if max_run_id is None:\n",
    "        next_id = 0\n",
    "    else:\n",
    "        next_id = (max_run_id + 1) if new_persist else max_run_id\n",
    "\n",
    "    persist_path = base_path / f\"{basename}_{next_id}\"\n",
    "\n",
    "    if create:\n",
    "        persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return persist_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ca18a",
   "metadata": {},
   "source": [
    "# 2D image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9312be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY starts with: sk-pr...\n",
      "Loaded 3 code documents from './stm_data_code_sample/2D_image_data' (.py + .ipynb).\n",
      "Split into 6 chunks.\n",
      "Created and persisted vector DB at WindowsPath('C:/Users/ggn/1_py_scripts/RAG/RAG_data_analysis_hackathon_2025/persists/chroma_db_code_7').\n"
     ]
    }
   ],
   "source": [
    "new_persist = True # Set to True to create a new persist directory\n",
    "folder_path = \"./stm_data_code_sample/2D_image_data\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Environment variables\n",
    "# -----------------------------\n",
    "# Make sure OPENAI_API_KEY is set in your environment before running\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "print(\"OPENAI_API_KEY starts with:\", (os.environ[\"OPENAI_API_KEY\"][:5] + \"...\") if os.environ[\"OPENAI_API_KEY\"] else \"Not Set\")\n",
    "\n",
    "# Disable LangSmith tracing (prevents 401 errors if you don't use LangSmith)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-code-search\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Text splitting / chunking (tuned for code)\n",
    "# -----------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 1200,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load .py and .ipynb from a folder\n",
    "# -----------------------------\n",
    "def load_py_file(path: Path) -> Document:\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"py\"})\n",
    "\n",
    "def load_ipynb_file(path: Path) -> Document:\n",
    "    nb = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "    parts = []\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        cell_type = cell.get(\"cell_type\", \"\")\n",
    "        src = cell.get(\"source\", [])\n",
    "        if isinstance(src, list):\n",
    "            src = \"\".join(src)\n",
    "        src = (src or \"\").strip()\n",
    "        if not src:\n",
    "            continue\n",
    "\n",
    "        # Keep both markdown + code, but label them\n",
    "        if cell_type == \"code\":\n",
    "            parts.append(\"# --- notebook code cell ---\\n\" + src)\n",
    "        elif cell_type == \"markdown\":\n",
    "            parts.append(\"# --- notebook markdown cell ---\\n\" + src)\n",
    "\n",
    "    text = \"\\n\\n\".join(parts)\n",
    "    return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"ipynb\"})\n",
    "\n",
    "def load_code_documents(folder_path: str) -> List[Document]:\n",
    "    folder = Path(folder_path)\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    for path in folder.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "\n",
    "        suffix = path.suffix.lower()\n",
    "        if suffix == \".py\":\n",
    "            documents.append(load_py_file(path))\n",
    "        elif suffix == \".ipynb\":\n",
    "            documents.append(load_ipynb_file(path))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "documents = load_code_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} code documents from {folder_path!r} (.py + .ipynb).\")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(splits)} chunks.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Embedding + Vector store (persisted)\n",
    "# -----------------------------\n",
    "#PERSIST_DIR = Path(\"./chroma_db_code_1\")\n",
    "PERSIST_DIR = get_persist_dir(\"./persists/\", \"chroma_db_code\", new_persist=new_persist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"code_collection\"\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# IMPORTANT: if you run from_documents every time, you rebuild the DB.\n",
    "# The logic below loads existing DB if present, otherwise builds it.\n",
    "db_file = Path(PERSIST_DIR) / \"chroma.sqlite3\"\n",
    "if db_file.exists():\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    print(f\"Loaded existing vector DB from {PERSIST_DIR!r}.\")\n",
    "else:\n",
    "    if not splits:\n",
    "        raise ValueError(\"No code chunks found. Check that ./docs contains .py or .ipynb files.\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embedding_function,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    #vectorstore.persist()\n",
    "    print(f\"Created and persisted vector DB at {PERSIST_DIR!r}.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Retriever\n",
    "# -----------------------------\n",
    "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 6, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) RAG chain that outputs PYTHON CODE ONLY (as a string)\n",
    "# -----------------------------\n",
    "def docs2str(docs: List[Document]) -> str:\n",
    "    # Keep source hints so the model can reference repo utilities if they exist\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        out.append(f\"### SOURCE: {src}\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "template = \"\"\"You are a coding assistant.\n",
    "\n",
    "You must write Python code ONLY (no markdown fences, no explanations).\n",
    "Your output must be a single Python script as plain text.\n",
    "\n",
    "Task:\n",
    "- Write code to plot the Z-height image of the file \"image.sxm\" located in \"./docs\".\n",
    "- If a library is needed, include a short install hint as a Python comment (e.g. # pip install ...).\n",
    "- Prefer using any existing loader/utility patterns found in the CONTEXT below.\n",
    "- The script should:\n",
    "  1) load ./docs/image.sxm\n",
    "  2) extract the Z/height channel (or the most appropriate topography channel)\n",
    "  3) plot it with matplotlib imshow + colorbar\n",
    "  4) save the figure to ./z_height.png and also show it\n",
    "\n",
    "CONTEXT (repository code snippets):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Python code:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5\", temperature= 0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fb24d",
   "metadata": {},
   "source": [
    "## RAG Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca22da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GENERATED PYTHON CODE (STRING) =====\n",
      "\n",
      "# pip install stmpy scikit-learn scipy matplotlib\n",
      "import os\n",
      "import sys\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Ensure we can import stm_utils from the repository structure\n",
      "repo_rel_utils_dir = os.path.join('.', 'stm_data_code_sample', '2D_image_data')\n",
      "if repo_rel_utils_dir not in sys.path:\n",
      "    sys.path.insert(0, repo_rel_utils_dir)\n",
      "\n",
      "try:\n",
      "    from stm_utils import Sxm_Image\n",
      "except Exception as e:\n",
      "    raise ImportError(\"Failed to import Sxm_Image from stm_utils. Ensure the path is correct and dependencies are installed. \"\n",
      "                      \"Try: pip install stmpy scikit-learn scipy matplotlib\") from e\n",
      "\n",
      "def select_height_channel(channels):\n",
      "    # Preference order and common aliases for topography/height channels\n",
      "    preferred = [\n",
      "        'Z_Fwd', 'Z_Bkd', 'Z_Forward', 'Z_Backward',\n",
      "        'Height', 'Topography', 'Topo', 'Z', 'ZSensor'\n",
      "    ]\n",
      "    ch_lower_map = {ch.lower(): ch for ch in channels}\n",
      "\n",
      "    # Exact preferred names (case-insensitive)\n",
      "    for name in preferred:\n",
      "        if name.lower() in ch_lower_map:\n",
      "            return ch_lower_map[name.lower()]\n",
      "\n",
      "    # Fallback: any channel containing common substrings\n",
      "    substrings = ['z', 'height', 'topo', 'topography']\n",
      "    for sub in substrings:\n",
      "        for ch in channels:\n",
      "            if sub in ch.lower():\n",
      "                return ch\n",
      "\n",
      "    # As a last resort, return the first available channel\n",
      "    return channels[0] if channels else None\n",
      "\n",
      "def main():\n",
      "    filepath = os.path.join('.', 'stm_data_code_sample', '2D_image_data', 'image.sxm')\n",
      "    if not os.path.isfile(filepath):\n",
      "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
      "\n",
      "    im = Sxm_Image(filepath)\n",
      "    channels = im.get_channels()\n",
      "\n",
      "    if not channels:\n",
      "        raise RuntimeError(\"No channels found in the SXM file.\")\n",
      "\n",
      "    channel = select_height_channel(channels)\n",
      "    if channel is None:\n",
      "        raise RuntimeError(\"Could not determine a suitable Z/height channel from the SXM file.\")\n",
      "\n",
      "    image = im.image(channel)\n",
      "\n",
      "    plt.figure()\n",
      "    plt.imshow(image, origin='lower')\n",
      "    plt.colorbar()\n",
      "    plt.title(channel)\n",
      "    plt.axis('off')\n",
      "    plt.show()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7) Ask the question\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "question = \"use the file at './stm_data_code_sample/2D_image_data/image.sxm' to plot the Z-height image of the image.sxm file. Don't save the figure\"\n",
    "response_code_str = rag_chain.invoke(question)\n",
    "\n",
    "print(\"\\n===== GENERATED PYTHON CODE (STRING) =====\\n\")\n",
    "print(response_code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200f42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4182deb4",
   "metadata": {},
   "source": [
    "# 3D hyperspectral data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3695f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY starts with: sk-pr...\n",
      "Loaded 2 code documents from './stm_data_code_sample/3D_hyperspectral_data' (.py + .ipynb).\n",
      "Split into 10 chunks.\n",
      "Created and persisted vector DB at WindowsPath('C:/Users/ggn/1_py_scripts/RAG/RAG_data_analysis_hackathon_2025/persists/chroma_db_code3d_1').\n"
     ]
    }
   ],
   "source": [
    "new_persist = True # Set to True to create a new persist directory\n",
    "folder_path = \"./stm_data_code_sample/3D_hyperspectral_data\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Environment variables\n",
    "# -----------------------------\n",
    "# Make sure OPENAI_API_KEY is set in your environment before running\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "print(\"OPENAI_API_KEY starts with:\", (os.environ[\"OPENAI_API_KEY\"][:5] + \"...\") if os.environ[\"OPENAI_API_KEY\"] else \"Not Set\")\n",
    "\n",
    "# Disable LangSmith tracing (prevents 401 errors if you don't use LangSmith)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-code-search\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Text splitting / chunking (tuned for code)\n",
    "# -----------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 800,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load .py and .ipynb from a folder\n",
    "# -----------------------------\n",
    "def load_py_file(path: Path) -> Document:\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"py\"})\n",
    "\n",
    "def load_ipynb_file(path: Path) -> Document:\n",
    "    nb = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "    parts = []\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        cell_type = cell.get(\"cell_type\", \"\")\n",
    "        src = cell.get(\"source\", [])\n",
    "        if isinstance(src, list):\n",
    "            src = \"\".join(src)\n",
    "        src = (src or \"\").strip()\n",
    "        if not src:\n",
    "            continue\n",
    "\n",
    "        # Keep both markdown + code, but label them\n",
    "        if cell_type == \"code\":\n",
    "            parts.append(\"# --- notebook code cell ---\\n\" + src)\n",
    "        elif cell_type == \"markdown\":\n",
    "            parts.append(\"# --- notebook markdown cell ---\\n\" + src)\n",
    "\n",
    "    text = \"\\n\\n\".join(parts)\n",
    "    return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"ipynb\"})\n",
    "\n",
    "def load_code_documents(folder_path: str) -> List[Document]:\n",
    "    folder = Path(folder_path)\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    for path in folder.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "\n",
    "        suffix = path.suffix.lower()\n",
    "        if suffix == \".py\":\n",
    "            documents.append(load_py_file(path))\n",
    "        elif suffix == \".ipynb\":\n",
    "            documents.append(load_ipynb_file(path))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "documents = load_code_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} code documents from {folder_path!r} (.py + .ipynb).\")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(splits)} chunks.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Embedding + Vector store (persisted)\n",
    "# -----------------------------\n",
    "#PERSIST_DIR = Path(\"./chroma_db_code_1\")\n",
    "PERSIST_DIR = get_persist_dir(\"./persists/\", \"chroma_db_code3d\", new_persist=new_persist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"code_collection\"\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# IMPORTANT: if you run from_documents every time, you rebuild the DB.\n",
    "# The logic below loads existing DB if present, otherwise builds it.\n",
    "db_file = Path(PERSIST_DIR) / \"chroma.sqlite3\"\n",
    "if db_file.exists():\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    print(f\"Loaded existing vector DB from {PERSIST_DIR!r}.\")\n",
    "else:\n",
    "    if not splits:\n",
    "        raise ValueError(\"No code chunks found. Check that ./docs contains .py or .ipynb files.\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embedding_function,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    #vectorstore.persist()\n",
    "    print(f\"Created and persisted vector DB at {PERSIST_DIR!r}.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Retriever\n",
    "# -----------------------------\n",
    "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 6, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) RAG chain that outputs PYTHON CODE ONLY (as a string)\n",
    "# -----------------------------\n",
    "def docs2str(docs: List[Document]) -> str:\n",
    "    # Keep source hints so the model can reference repo utilities if they exist\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        out.append(f\"### SOURCE: {src}\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "template = \"\"\"You are a coding assistant.\n",
    "\n",
    "You must write Python code ONLY (no markdown fences, no explanations).\n",
    "Your output must be a single Python script as plain text.\n",
    "\n",
    "Task:\n",
    "- Write code to plot the Z-height image of the file \"image.sxm\" located in \"./docs\".\n",
    "- If a library is needed, include a short install hint as a Python comment (e.g. # pip install ...).\n",
    "- Prefer using any existing loader/utility patterns found in the CONTEXT below.\n",
    "- The script should:\n",
    "  1) load ./docs/image.sxm\n",
    "  2) extract the Z/height channel (or the most appropriate topography channel)\n",
    "  3) plot it with matplotlib imshow + colorbar\n",
    "  4) save the figure to ./z_height.png and also show it\n",
    "\n",
    "CONTEXT (repository code snippets):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Python code:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5\", temperature= 0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5345765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GENERATED PYTHON CODE (STRING) =====\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# pip install hoffmanstmpy\n",
      "\n",
      "try:\n",
      "    import stmpy\n",
      "except ImportError as e:\n",
      "    raise ImportError(\"stmpy is required. Install via: pip install hoffmanstmpy\") from e\n",
      "\n",
      "def load_cits_class_module(module_path):\n",
      "    import importlib.util\n",
      "    spec = importlib.util.spec_from_file_location(\"CITS_Class\", module_path)\n",
      "    if spec is None or spec.loader is None:\n",
      "        raise ImportError(f\"Cannot load module from {module_path}\")\n",
      "    module = importlib.util.module_from_spec(spec)\n",
      "    spec.loader.exec_module(module)\n",
      "    # Ensure stmpy is available to the module if it relies on a global reference\n",
      "    setattr(module, \"stmpy\", stmpy)\n",
      "    # Ensure numpy if needed\n",
      "    if not hasattr(module, \"np\"):\n",
      "        setattr(module, \"np\", np)\n",
      "    return module\n",
      "\n",
      "def main():\n",
      "    repo_root = os.path.abspath(\".\")\n",
      "    data_path = os.path.join(repo_root, \"stm_data_code_sample\", \"3D_hyperspectral_data\", \"cits_data.3ds\")\n",
      "    if not os.path.isfile(data_path):\n",
      "        raise FileNotFoundError(f\"Could not find data file at: {data_path}\")\n",
      "\n",
      "    cits_module_path = os.path.join(repo_root, \"stm_data_code_sample\", \"3D_hyperspectral_data\", \"CITS_Class.py\")\n",
      "    if not os.path.isfile(cits_module_path):\n",
      "        raise FileNotFoundError(f\"Could not find CITS_Class.py at: {cits_module_path}\")\n",
      "\n",
      "    CITS_Module = load_cits_class_module(cits_module_path)\n",
      "    if not hasattr(CITS_Module, \"CITS_Analysis\"):\n",
      "        raise AttributeError(\"CITS_Analysis class not found in CITS_Class.py\")\n",
      "\n",
      "    data = CITS_Module.CITS_Analysis(data_path)\n",
      "\n",
      "    probe_bias = 1.2\n",
      "    i_2D, V_actual = data.current_map(probe_bias)\n",
      "    print(\"Nearest Probed bias = \", V_actual)\n",
      "\n",
      "    plt.figure(figsize=(6, 5))\n",
      "    plt.imshow(i_2D, aspect='auto', origin='lower')\n",
      "    plt.title(f\"Current Map at {V_actual:.3f} V\")\n",
      "    plt.xlabel(\"X index\")\n",
      "    plt.ylabel(\"Y index\")\n",
      "    plt.colorbar(label=\"Current (arb. units)\")\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7) Ask the question\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "question = \"use the file at './stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds' to plot the current map image at a bias of 1.2 V. Don't save the figure\"\n",
    "response_code_str = rag_chain.invoke(question)\n",
    "\n",
    "print(\"\\n===== GENERATED PYTHON CODE (STRING) =====\\n\")\n",
    "print(response_code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef887d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afac73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_env_kernel",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
