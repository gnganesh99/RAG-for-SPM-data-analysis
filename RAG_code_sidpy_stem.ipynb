{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8b13da",
   "metadata": {},
   "source": [
    "# RAG for more complex microscope codebase\n",
    "\n",
    "Here we developed a RAG agent to interface with [sidpy](https://pycroscopy.github.io/sidpy/), a dask-based python framework for microscopy data.  \n",
    "\n",
    "The user interacts with the agent to perform basic operations to convert their data structures into sidpy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fdc46",
   "metadata": {},
   "source": [
    "# Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f563a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144b4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "718af2a6",
   "metadata": {},
   "source": [
    "## Function for persist_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f910600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def get_persist_dir(\n",
    "    base_dir: str | Path,\n",
    "    basename: str,\n",
    "    *,\n",
    "    new_persist: bool = False,\n",
    "    create: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Find (or create) a persist directory under base_dir.\n",
    "\n",
    "    Looks for directories named: f\"{basename}_{run_id}\" where run_id is an int.\n",
    "    - If new_persist=False: returns the latest existing folder if found, else basename_0\n",
    "    - If new_persist=True : returns a new folder with run_id = (latest + 1) or 0 if none exist\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir).expanduser().resolve()\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pattern = re.compile(rf\"^{re.escape(basename)}_(\\d+)$\")\n",
    "\n",
    "    max_run_id: Optional[int] = None\n",
    "    for p in base_path.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        run_id = int(m.group(1))\n",
    "        if max_run_id is None or run_id > max_run_id:\n",
    "            max_run_id = run_id\n",
    "\n",
    "    if max_run_id is None:\n",
    "        next_id = 0\n",
    "    else:\n",
    "        next_id = (max_run_id + 1) if new_persist else max_run_id\n",
    "\n",
    "    persist_path = base_path / f\"{basename}_{next_id}\"\n",
    "\n",
    "    if create:\n",
    "        persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return persist_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65a218",
   "metadata": {},
   "source": [
    "## RAG chain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "id": "1905206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d401b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(folder_path: str, persist_basename: str, \n",
    "                  new_persist: bool = False, chunk_size: int = 1200, chunk_overlap: int = 200, \n",
    "                  model: str = \"gpt-5\", temperature: int = 0, api_key: Optional[str] = None):\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING get_rag_chain\")\n",
    "    print(f\"Parameters: folder_path={folder_path}, persist_basename={persist_basename}\")\n",
    "    print(f\"new_persist={new_persist}, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
    "    print(f\"model={model}, temperature={temperature}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Environment variables\n",
    "    # -----------------------------\n",
    "    print(\"\\n[1/6] Setting up environment variables...\")\n",
    "    # Make sure OPENAI_API_KEY is set in your environment before running\n",
    "    if api_key is None:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "    else:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    print(\"OPENAI_API_KEY starts with:\", (os.environ[\"OPENAI_API_KEY\"][:5] + \"...\") if os.environ[\"OPENAI_API_KEY\"] else \"Not Set\")\n",
    "\n",
    "    # Disable LangSmith tracing (prevents 401 errors if you don't use LangSmith)\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-code-search\"\n",
    "    print(\"✓ Environment variables configured\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Text splitting / chunking (tuned for code)\n",
    "    # -----------------------------\n",
    "    print(\"\\n[2/6] Creating text splitter...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size= chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    print(\"✓ Text splitter created\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Load .py and .ipynb from a folder\n",
    "    # -----------------------------\n",
    "    print(\"\\n[3/6] Loading code documents...\")\n",
    "    \n",
    "    def load_py_file(path: Path) -> Document:\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"py\"})\n",
    "\n",
    "    def load_ipynb_file(path: Path) -> Document:\n",
    "        nb = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "        parts = []\n",
    "        for cell in nb.get(\"cells\", []):\n",
    "            cell_type = cell.get(\"cell_type\", \"\")\n",
    "            src = cell.get(\"source\", [])\n",
    "            if isinstance(src, list):\n",
    "                src = \"\".join(src)\n",
    "            src = (src or \"\").strip()\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            # Keep both markdown + code, but label them\n",
    "            if cell_type == \"code\":\n",
    "                parts.append(\"# --- notebook code cell ---\\n\" + src)\n",
    "            elif cell_type == \"markdown\":\n",
    "                parts.append(\"# --- notebook markdown cell ---\\n\" + src)\n",
    "\n",
    "        text = \"\\n\\n\".join(parts)\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"ipynb\"})\n",
    "\n",
    "    def load_code_documents(folder_path: str) -> List[Document]:\n",
    "        folder = Path(folder_path)\n",
    "        documents: List[Document] = []\n",
    "\n",
    "        for path in folder.rglob(\"*\"):\n",
    "            if path.is_dir():\n",
    "                continue\n",
    "\n",
    "            suffix = path.suffix.lower()\n",
    "            if suffix == \".py\":\n",
    "                documents.append(load_py_file(path))\n",
    "            elif suffix == \".ipynb\":\n",
    "                documents.append(load_ipynb_file(path))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "\n",
    "    documents = load_code_documents(folder_path)\n",
    "    print(f\"✓ Loaded {len(documents)} code documents from {folder_path!r} (.py + .ipynb).\")\n",
    "\n",
    "    print(\"  Splitting documents into chunks...\")\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"✓ Split into {len(splits)} chunks.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Embedding + Vector store (persisted)\n",
    "    # -----------------------------\n",
    "    print(\"\\n[4/6] Setting up vector store...\")\n",
    "    #PERSIST_DIR = Path(\"./chroma_db_code_1\")\n",
    "    PERSIST_DIR = get_persist_dir(\"./persists/\", persist_basename, new_persist=new_persist)\n",
    "    print(f\"  Persist directory: {PERSIST_DIR}\")\n",
    "\n",
    "    COLLECTION_NAME = \"code_collection\"\n",
    "\n",
    "    print(\"  Creating embedding function...\")\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    print(\"✓ Embedding function created\")\n",
    "\n",
    "    # IMPORTANT: if you run from_documents every time, you rebuild the DB.\n",
    "    # The logic below loads existing DB if present, otherwise builds it.\n",
    "    db_file = Path(PERSIST_DIR) # / \"chroma.sqlite3\"\n",
    "    if db_file.exists():\n",
    "        print(f\"  Loading existing vector DB from {PERSIST_DIR}...\")\n",
    "        print(\"  ⚠️  WARNING: If on OneDrive, this may cause kernel crash!\")\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        print(f\"✓ Loaded existing vector DB from {PERSIST_DIR!r}.\")\n",
    "    else:\n",
    "        if not splits:\n",
    "            raise ValueError(\"No code chunks found. Check that ./docs contains .py or .ipynb files.\")\n",
    "        print(f\"  Creating new vector DB (this may take a moment)...\")\n",
    "        print(\"  ⚠️  WARNING: If on OneDrive, this may cause kernel crash!\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_function,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "        )\n",
    "        #vectorstore.persist()\n",
    "        print(f\"✓ Created and persisted vector DB at {PERSIST_DIR!r}.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Retriever\n",
    "    # -----------------------------\n",
    "    print(\"\\n[5/6] Creating retriever...\")\n",
    "    #retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 6, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    "    )\n",
    "    print(\"✓ Retriever created with MMR search (k=6, fetch_k=20)\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) RAG chain that outputs PYTHON CODE ONLY (as a string)\n",
    "    # -----------------------------\n",
    "    print(\"\\n[6/6] Building RAG chain...\")\n",
    "    \n",
    "    def docs2str(docs: List[Document]) -> str:\n",
    "        # Keep source hints so the model can reference repo utilities if they exist\n",
    "        out = []\n",
    "        for d in docs:\n",
    "            src = d.metadata.get(\"source\", \"unknown\")\n",
    "            out.append(f\"### SOURCE: {src}\\n{d.page_content}\")\n",
    "        return \"\\n\\n\".join(out)\n",
    "\n",
    "    template = \"\"\"You are a coding assistant.\n",
    "\n",
    "    You must write Python code ONLY (no markdown fences, no explanations).\n",
    "    Your output must be a single Python script as plain text.\n",
    "\n",
    "    CONTEXT (repository code snippets):\n",
    "    {context}\n",
    "\n",
    "    Task:\n",
    "    {question}\n",
    "\n",
    "    Python code:\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    print(\"  Prompt template created\")\n",
    "\n",
    "    print(f\"  Initializing LLM ({model})...\")\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "    print(\"✓ LLM initialized\")\n",
    "\n",
    "    print(\"  Assembling chain...\")\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"✓ RAG chain assembled\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓✓✓ get_rag_chain completed successfully!\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa506a9",
   "metadata": {},
   "source": [
    "## Write code to py-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37385319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_generated_code_to_file(\n",
    "    code_str: str,\n",
    "    prompt_str: str = \"\",\n",
    "    filename: str = \"main_rag_test.py\",\n",
    "    encoding: str = \"utf-8\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Overwrite `filename` with `code_str`. Creates the file if it doesn't exist.\n",
    "    Returns the Path to the written file.\n",
    "    \"\"\"\n",
    "    path = Path(filename).expanduser().resolve()\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure we're writing a string (some chains return dicts / messages)\n",
    "    if not isinstance(code_str, str):\n",
    "        code_str = str(code_str)\n",
    "\n",
    "    # Add a trailing newline for nicer diffs/editors\n",
    "    if not code_str.endswith(\"\\n\"):\n",
    "        code_str += \"\\n\"\n",
    "\n",
    "    path.write_text(f\"# Prompt: \\n \\\"\\\"\\\" {prompt_str} \\n \\\"\\\"\\\" \\n {code_str}\", encoding = encoding)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ca18a",
   "metadata": {},
   "source": [
    "# learn sidpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9312be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_persist = False # Set to True to create a new persist directory. Set to True while creating new db.\n",
    "folder_path = \"C:/Users/zwx/Documents/sidpy\"\n",
    "\n",
    "chunk_size = 800\n",
    "chunk_overlap = 100\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "persist_basename = \"chroma_db_sidpy\"\n",
    "\n",
    "model = \"gpt-5\"\n",
    "temperature = 0\n",
    "\n",
    "task = \"\"\"\n",
    "Write code to generate a sidpy dataset for a fake 2D image of size 256x256.\\n\n",
    "You can fill the image with random values from 0 to 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bc449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING get_rag_chain\n",
      "Parameters: folder_path=C:/Users/zwx/Documents/sidpy, persist_basename=chroma_db_sidpy\n",
      "new_persist=False, chunk_size=800, chunk_overlap=100\n",
      "model=gpt-5, temperature=0\n",
      "============================================================\n",
      "\n",
      "[1/6] Setting up environment variables...\n",
      "OPENAI_API_KEY starts with: sk-pr...\n",
      "✓ Environment variables configured\n",
      "\n",
      "[2/6] Creating text splitter...\n",
      "✓ Text splitter created\n",
      "\n",
      "[3/6] Loading code documents...\n",
      "✓ Loaded 101 code documents from 'C:/Users/zwx/Documents/sidpy' (.py + .ipynb).\n",
      "  Splitting documents into chunks...\n",
      "✓ Split into 2466 chunks.\n",
      "\n",
      "[4/6] Setting up vector store...\n",
      "  Persist directory: C:\\Users\\zwx\\Documents\\RAG-for-SPM-data-analysis\\persists\\chroma_db_sidpy_0\n",
      "  Creating embedding function...\n",
      "✓ Embedding function created\n",
      "  Loading existing vector DB from C:\\Users\\zwx\\Documents\\RAG-for-SPM-data-analysis\\persists\\chroma_db_sidpy_0...\n",
      "  ⚠️  WARNING: If on OneDrive, this may cause kernel crash!\n",
      "✓ Loaded existing vector DB from WindowsPath('C:/Users/zwx/Documents/RAG-for-SPM-data-analysis/persists/chroma_db_sidpy_0').\n",
      "\n",
      "[5/6] Creating retriever...\n",
      "✓ Retriever created with MMR search (k=6, fetch_k=20)\n",
      "\n",
      "[6/6] Building RAG chain...\n",
      "  Prompt template created\n",
      "  Initializing LLM (gpt-5)...\n",
      "✓ LLM initialized\n",
      "  Assembling chain...\n",
      "✓ RAG chain assembled\n",
      "\n",
      "============================================================\n",
      "✓✓✓ get_rag_chain completed successfully!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_chain = get_rag_chain(\n",
    "    folder_path=folder_path,\n",
    "    persist_basename=persist_basename,\n",
    "    new_persist=new_persist,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d04a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57021c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000018B3636AE10>, search_type='mmr', search_kwargs={'k': 6, 'fetch_k': 20, 'lambda_mult': 0.5})\n",
       "           | RunnableLambda(docs2str),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a coding assistant.\\n\\n    You must write Python code ONLY (no markdown fences, no explanations).\\n    Your output must be a single Python script as plain text.\\n\\n    CONTEXT (repository code snippets):\\n    {context}\\n\\n    Task:\\n    {question}\\n\\n    Python code:'), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 400000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018B3641E450>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018B3641F0D0>, root_client=<openai.OpenAI object at 0x0000018B3641E010>, root_async_client=<openai.AsyncOpenAI object at 0x0000018B363D3A50>, model_name='gpt-5', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c0eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rag_chain created successfully\n",
      "\n",
      "============================================================\n",
      "INVOKING RAG CHAIN\n",
      "============================================================\n",
      "Task: \n",
      "Write code to generate a sidpy dataset for a fake 2D image of size 256x256.\n",
      "\n",
      "You can fill the image...\n",
      "\n",
      "[Step 1/3] Retrieving relevant documents from vector store...\n",
      "  ⚠️  WARNING: ChromaDB query on OneDrive may crash kernel here!\n",
      "\n",
      "[Step 2/3] Calling LLM to generate code...\n",
      "  Using model: gpt-5\n",
      "  This may take 10-30 seconds depending on complexity...\n",
      "\n",
      "[Step 3/3] ✓ Response received successfully!\n",
      "  Generated code length: 1796 characters\n",
      "\n",
      "===== GENERATED PYTHON CODE (STRING) =====\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Create random 2D image data\n",
      "np.random.seed(0)\n",
      "image = np.random.rand(256, 256)\n",
      "\n",
      "# Import sidpy Dataset and Dimension with fallbacks for different package structures\n",
      "try:\n",
      "    from sidpy import Dataset, Dimension\n",
      "except Exception:\n",
      "    try:\n",
      "        from sidpy.sid.dataset import Dataset\n",
      "        from sidpy.sid.dimension import Dimension\n",
      "    except Exception as e:\n",
      "        raise ImportError(\"sidpy is required to run this script. Please install sidpy.\") from e\n",
      "\n",
      "# Create the sidpy Dataset\n",
      "if hasattr(Dataset, 'from_array'):\n",
      "    ds = Dataset.from_array(image, name='Fake 2D Image')\n",
      "else:\n",
      "    ds = Dataset(image)\n",
      "    if hasattr(ds, 'title'):\n",
      "        ds.title = 'Fake 2D Image'\n",
      "    elif hasattr(ds, 'name'):\n",
      "        ds.name = 'Fake 2D Image'\n",
      "\n",
      "# Set dataset attributes\n",
      "if hasattr(ds, 'data_type'):\n",
      "    ds.data_type = 'image'\n",
      "if hasattr(ds, 'units'):\n",
      "    ds.units = 'a.u.'\n",
      "if hasattr(ds, 'quantity'):\n",
      "    ds.quantity = 'intensity'\n",
      "\n",
      "# Define and set dimensions\n",
      "y_vals = np.arange(image.shape[0])\n",
      "x_vals = np.arange(image.shape[1])\n",
      "\n",
      "dim_y = Dimension(y_vals, name='Y', units='pixel', quantity='position', dimension_type='spatial')\n",
      "dim_x = Dimension(x_vals, name='X', units='pixel', quantity='position', dimension_type='spatial')\n",
      "\n",
      "if hasattr(ds, 'set_dimension'):\n",
      "    ds.set_dimension(0, dim_y)\n",
      "    ds.set_dimension(1, dim_x)\n",
      "elif hasattr(ds, 'add_dimension'):\n",
      "    ds.add_dimension(dim_y)\n",
      "    ds.add_dimension(dim_x)\n",
      "\n",
      "# Example usage: print a brief summary\n",
      "try:\n",
      "    print(ds)\n",
      "except Exception:\n",
      "    print(\"Created sidpy Dataset:\")\n",
      "    print(f\"  Name/Title: {getattr(ds, 'title', getattr(ds, 'name', 'Unknown'))}\")\n",
      "    print(f\"  Shape: {getattr(ds, 'shape', None)}\")\n",
      "    print(f\"  Data type: {getattr(ds, 'data_type', 'Unknown')}\")\n",
      "    print(f\"  Units: {getattr(ds, 'units', 'Unknown')}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('✓ rag_chain created successfully')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INVOKING RAG CHAIN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Task: {task[:100]}...\" if len(task) > 100 else f\"Task: {task}\")\n",
    "print(\"\\n[Step 1/3] Retrieving relevant documents from vector store...\")\n",
    "print(\"  ⚠️  WARNING: ChromaDB query on OneDrive may crash kernel here!\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n[Step 2/3] Calling LLM to generate code...\")\n",
    "    print(f\"  Using model: {model}\")\n",
    "    print(\"  This may take 10-30 seconds depending on complexity...\")\n",
    "    \n",
    "    response_code_str = rag_chain.invoke(task)\n",
    "    \n",
    "    print(\"\\n[Step 3/3] ✓ Response received successfully!\")\n",
    "    print(f\"  Generated code length: {len(response_code_str)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n❌ ERROR during rag_chain.invoke()!\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\n===== GENERATED PYTHON CODE (STRING) =====\\n\")\n",
    "print(response_code_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81b30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== WRITING GENERATED CODE TO FILE =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== WRITING GENERATED CODE TO FILE =====\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbf8a8",
   "metadata": {},
   "source": [
    "## Test the code\n",
    "Write the code to \"main_rag_test.py\". Overwrites if already exists.\n",
    "\n",
    "Test it by running \"python main_rag_test.py\" on terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b5e9d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/zwx/Documents/RAG-for-SPM-data-analysis/test_sidpy_rag.py')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_generated_code_to_file(\n",
    "    code_str=response_code_str,\n",
    "    prompt_str=task,\n",
    "    filename=\"test_sidpy_rag.py\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon_rag (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
