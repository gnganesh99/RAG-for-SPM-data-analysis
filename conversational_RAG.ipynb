{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8b13da",
   "metadata": {},
   "source": [
    "# Conversational RAG for STM data analysis\n",
    "\n",
    "Here we developed a RAG agent to interface with custom data analysis code. \n",
    "\n",
    "The user interacts with the agent to analyse STM data. The agent then provides new code that can be used for plotting data.\n",
    "\n",
    "We use the \"test_rag_output.ipynb\" to the RAG prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fdc46",
   "metadata": {},
   "source": [
    "# Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f563a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e23a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_classic.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718af2a6",
   "metadata": {},
   "source": [
    "## Function for persist_db dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def get_persist_dir(\n",
    "    base_dir: str | Path,\n",
    "    basename: str,\n",
    "    *,\n",
    "    new_persist: bool = False,\n",
    "    create: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Find (or create) a persist directory under base_dir.\n",
    "\n",
    "    Looks for directories named: f\"{basename}_{run_id}\" where run_id is an int.\n",
    "    - If new_persist=False: returns the latest existing folder if found, else basename_0\n",
    "    - If new_persist=True : returns a new folder with run_id = (latest + 1) or 0 if none exist\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir).expanduser().resolve()\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pattern = re.compile(rf\"^{re.escape(basename)}_(\\d+)$\")\n",
    "\n",
    "    max_run_id: Optional[int] = None\n",
    "    for p in base_path.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        run_id = int(m.group(1))\n",
    "        if max_run_id is None or run_id > max_run_id:\n",
    "            max_run_id = run_id\n",
    "\n",
    "    if max_run_id is None:\n",
    "        next_id = 0\n",
    "    else:\n",
    "        next_id = (max_run_id + 1) if new_persist else max_run_id\n",
    "\n",
    "    persist_path = base_path / f\"{basename}_{next_id}\"\n",
    "\n",
    "    if create:\n",
    "        persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return persist_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65a218",
   "metadata": {},
   "source": [
    "## RAG chain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d401b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(folder_path: str, persist_basename: str, \n",
    "                  new_persist: bool = False, chunk_size: int = 1200, chunk_overlap: int = 200, \n",
    "                  model: str = \"gpt-5\", temperature: float = 0.0, api_key: Optional[str] = None):\n",
    "    \n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Environment variables\n",
    "    # -----------------------------\n",
    "    # Make sure OPENAI_API_KEY is set in your environment before running\n",
    "    if api_key is None:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "    else:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    print(\"OPENAI_API_KEY starts with:\", (os.environ[\"OPENAI_API_KEY\"][:5] + \"...\") if os.environ[\"OPENAI_API_KEY\"] else \"Not Set\")\n",
    "\n",
    "    # Disable LangSmith tracing (prevents 401 errors if you don't use LangSmith)\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-code-search\"\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Text splitting / chunking (tuned for code)\n",
    "    # -----------------------------\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size= chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Load .py and .ipynb from a folder\n",
    "    # -----------------------------\n",
    "    def load_py_file(path: Path) -> Document:\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"py\"})\n",
    "\n",
    "    def load_ipynb_file(path: Path) -> Document:\n",
    "        nb = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "        parts = []\n",
    "        for cell in nb.get(\"cells\", []):\n",
    "            cell_type = cell.get(\"cell_type\", \"\")\n",
    "            src = cell.get(\"source\", [])\n",
    "            if isinstance(src, list):\n",
    "                src = \"\".join(src)\n",
    "            src = (src or \"\").strip()\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            # Keep both markdown + code, but label them\n",
    "            if cell_type == \"code\":\n",
    "                parts.append(\"# --- notebook code cell ---\\n\" + src)\n",
    "            elif cell_type == \"markdown\":\n",
    "                parts.append(\"# --- notebook markdown cell ---\\n\" + src)\n",
    "\n",
    "        text = \"\\n\\n\".join(parts)\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"ipynb\"})\n",
    "\n",
    "    def load_code_documents(folder_path: str) -> List[Document]:\n",
    "        folder = Path(folder_path)\n",
    "        documents: List[Document] = []\n",
    "\n",
    "        for path in folder.rglob(\"*\"):\n",
    "            if path.is_dir():\n",
    "                continue\n",
    "\n",
    "            suffix = path.suffix.lower()\n",
    "            if suffix == \".py\":\n",
    "                documents.append(load_py_file(path))\n",
    "            elif suffix == \".ipynb\":\n",
    "                documents.append(load_ipynb_file(path))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "\n",
    "    documents = load_code_documents(folder_path)\n",
    "    print(f\"Loaded {len(documents)} code documents from {folder_path!r} (.py + .ipynb).\")\n",
    "\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(splits)} chunks.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Embedding + Vector store (persisted)\n",
    "    # -----------------------------\n",
    "    #PERSIST_DIR = Path(\"./chroma_db_code_1\")\n",
    "    PERSIST_DIR = get_persist_dir(\"./persists/\", persist_basename, new_persist=new_persist)\n",
    "\n",
    "\n",
    "    COLLECTION_NAME = \"code_collection\"\n",
    "\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "    # IMPORTANT: if you run from_documents every time, you rebuild the DB.\n",
    "    # The logic below loads existing DB if present, otherwise builds it.\n",
    "    db_file = Path(PERSIST_DIR) / \"chroma.sqlite3\"\n",
    "    if db_file.exists():\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        print(f\"Loaded existing vector DB from {PERSIST_DIR!r}.\")\n",
    "    else:\n",
    "        if not splits:\n",
    "            raise ValueError(\"No code chunks found. Check that ./docs contains .py or .ipynb files.\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_function,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "        )\n",
    "        #vectorstore.persist()\n",
    "        print(f\"Created and persisted vector DB at {PERSIST_DIR!r}.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Retriever\n",
    "    # -----------------------------\n",
    "    #retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 6, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) RAG chain that outputs PYTHON CODE ONLY (as a string)\n",
    "    # -----------------------------\n",
    "    def docs2str(docs: List[Document]) -> str:\n",
    "        # Keep source hints so the model can reference repo utilities if they exist\n",
    "        out = []\n",
    "        for d in docs:\n",
    "            src = d.metadata.get(\"source\", \"unknown\")\n",
    "            out.append(f\"### SOURCE: {src}\\n{d.page_content}\")\n",
    "        return \"\\n\\n\".join(out)\n",
    "\n",
    "    template = \"\"\"You are a coding assistant.\n",
    "\n",
    "    You must write Python code ONLY (no markdown fences, no explanations).\n",
    "    Your output must be a single Python script as plain text.\n",
    "\n",
    "    CONTEXT (repository code snippets):\n",
    "    {context}\n",
    "\n",
    "    Task:\n",
    "    {question}\n",
    "\n",
    "    Python code:\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271343a8",
   "metadata": {},
   "source": [
    "## QA rag chain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_qa_rag_chain(folder_path: str, persist_basename: str, \n",
    "                  new_persist: bool = False, chunk_size: int = 1200, chunk_overlap: int = 200, \n",
    "                  model: str = \"gpt-5\", temperature: float = 0.0, api_key: Optional[str] = None):\n",
    "    \n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Environment variables\n",
    "    # -----------------------------\n",
    "    # Make sure OPENAI_API_KEY is set in your environment before running\n",
    "    if api_key is None:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "    else:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    print(\"OPENAI_API_KEY starts with:\", (os.environ[\"OPENAI_API_KEY\"][:5] + \"...\") if os.environ[\"OPENAI_API_KEY\"] else \"Not Set\")\n",
    "\n",
    "    # Disable LangSmith tracing (prevents 401 errors if you don't use LangSmith)\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-code-search\"\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Text splitting / chunking (tuned for code)\n",
    "    # -----------------------------\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size= chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Load .py and .ipynb from a folder\n",
    "    # -----------------------------\n",
    "    def load_py_file(path: Path) -> Document:\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"py\"})\n",
    "\n",
    "    def load_ipynb_file(path: Path) -> Document:\n",
    "        nb = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "        parts = []\n",
    "        for cell in nb.get(\"cells\", []):\n",
    "            cell_type = cell.get(\"cell_type\", \"\")\n",
    "            src = cell.get(\"source\", [])\n",
    "            if isinstance(src, list):\n",
    "                src = \"\".join(src)\n",
    "            src = (src or \"\").strip()\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            # Keep both markdown + code, but label them\n",
    "            if cell_type == \"code\":\n",
    "                parts.append(\"# --- notebook code cell ---\\n\" + src)\n",
    "            elif cell_type == \"markdown\":\n",
    "                parts.append(\"# --- notebook markdown cell ---\\n\" + src)\n",
    "\n",
    "        text = \"\\n\\n\".join(parts)\n",
    "        return Document(page_content=text, metadata={\"source\": str(path), \"type\": \"ipynb\"})\n",
    "\n",
    "    def load_code_documents(folder_path: str) -> List[Document]:\n",
    "        folder = Path(folder_path)\n",
    "        documents: List[Document] = []\n",
    "\n",
    "        for path in folder.rglob(\"*\"):\n",
    "            if path.is_dir():\n",
    "                continue\n",
    "\n",
    "            suffix = path.suffix.lower()\n",
    "            if suffix == \".py\":\n",
    "                documents.append(load_py_file(path))\n",
    "            elif suffix == \".ipynb\":\n",
    "                documents.append(load_ipynb_file(path))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "\n",
    "    documents = load_code_documents(folder_path)\n",
    "    print(f\"Loaded {len(documents)} code documents from {folder_path!r} (.py + .ipynb).\")\n",
    "\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(splits)} chunks.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Embedding + Vector store (persisted)\n",
    "    # -----------------------------\n",
    "    #PERSIST_DIR = Path(\"./chroma_db_code_1\")\n",
    "    PERSIST_DIR = get_persist_dir(\"./persists/\", persist_basename, new_persist=new_persist)\n",
    "\n",
    "\n",
    "    COLLECTION_NAME = \"code_collection\"\n",
    "\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "    # IMPORTANT: if you run from_documents every time, you rebuild the DB.\n",
    "    # The logic below loads existing DB if present, otherwise builds it.\n",
    "    db_file = Path(PERSIST_DIR) / \"chroma.sqlite3\"\n",
    "    if db_file.exists():\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        print(f\"Loaded existing vector DB from {PERSIST_DIR!r}.\")\n",
    "    else:\n",
    "        if not splits:\n",
    "            raise ValueError(\"No code chunks found. Check that ./docs contains .py or .ipynb files.\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_function,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "        )\n",
    "        #vectorstore.persist()\n",
    "        print(f\"Created and persisted vector DB at {PERSIST_DIR!r}.\")\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Retriever\n",
    "    # -----------------------------\n",
    "    #retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 6, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) RAG chain that outputs PYTHON CODE ONLY (as a string)\n",
    "    # -----------------------------\n",
    "    def docs2str(docs: List[Document]) -> str:\n",
    "        # Keep source hints so the model can reference repo utilities if they exist\n",
    "        out = []\n",
    "        for d in docs:\n",
    "            src = d.metadata.get(\"source\", \"unknown\")\n",
    "            out.append(f\"### SOURCE: {src}\\n{d.page_content}\")\n",
    "        return \"\\n\\n\".join(out)\n",
    "\n",
    "    \n",
    "    llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7 Query prompt (history -> standalone retrieval query)\n",
    "    # -----------------------------\n",
    "    contextualize_q_system_prompt = \"\"\"\n",
    "    You are a query rewriting assistant for retrieval.\n",
    "\n",
    "    Given the chat history and the user's latest question:\n",
    "    - Rewrite the question into a standalone search query that can be used to retrieve relevant code/docs.\n",
    "    - Resolve references like \"it\", \"that\", \"the previous one\", etc. using the chat history.\n",
    "    - Preserve exact identifiers (function names, class names, file paths, error messages, config keys).\n",
    "    - Do NOT answer the question. Only output the rewritten query.\n",
    "    - If the question is already standalone, return it unchanged.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        prompt=contextualize_q_prompt,\n",
    "    )\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8) Answer prompt (use retrieved docs + chat history)\n",
    "    # -----------------------------\n",
    "    qa_system_prompt = \"\"\"\n",
    "    You are a helpful assistant.\n",
    "\n",
    "    Use BOTH:\n",
    "    - The retrieved context (primary source of truth).\n",
    "    - The chat history (to understand intent, constraints, and resolve references).\n",
    "\n",
    "    Rules:\n",
    "    - Output only the python code as a single script, such the user can run it directly. Do not add ```python``` markers\n",
    "    - If using explanations or markdown text, comment them.\n",
    "    - Prefer the retrieved context for factual claims about the code/docs.\n",
    "    - Use chat history mainly to interpret what the user means and what constraints they set earlier.\n",
    "    - If the retrieved context does not contain the answer, say so clearly and suggest what to search for next.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"Question: {input}\\n\\nRetrieved context:\\n{context}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9) Full RAG chain\n",
    "    # -----------------------------\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23cf51",
   "metadata": {},
   "source": [
    "## DB connection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9fe235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "DB_NAME = \"rag_chat_data.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                     session_id TEXT,\n",
    "                     user_query TEXT,\n",
    "                     gpt_response TEXT,\n",
    "                     model TEXT,\n",
    "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "        HumanMessage(content=row['user_query']),\n",
    "        AIMessage(content=row['gpt_response'])\n",
    "])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the database\n",
    "create_application_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa506a9",
   "metadata": {},
   "source": [
    "## Write code to py-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37385319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_generated_code_to_file(\n",
    "    code_str: str,\n",
    "    filename: str = \"main_rag_test.py\",\n",
    "    encoding: str = \"utf-8\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Overwrite `filename` with `code_str`. Creates the file if it doesn't exist.\n",
    "    Returns the Path to the written file.\n",
    "    \"\"\"\n",
    "    path = Path(filename).expanduser().resolve()\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure we're writing a string (some chains return dicts / messages)\n",
    "    if not isinstance(code_str, str):\n",
    "        code_str = str(code_str)\n",
    "\n",
    "    # Add a trailing newline for nicer diffs/editors\n",
    "    if not code_str.endswith(\"\\n\"):\n",
    "        code_str += \"\\n\"\n",
    "\n",
    "    path.write_text(code_str, encoding=encoding)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182deb4",
   "metadata": {},
   "source": [
    "# Coversational data analysis\n",
    "\n",
    "Here we analyse the hyperspectral data in \"./stm_data_code_sample/3D_hyperspectral_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d38721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY starts with: sk-pr...\n",
      "Loaded 2 code documents from './stm_data_code_sample/3D_hyperspectral_data' (.py + .ipynb).\n",
      "Split into 7 chunks.\n",
      "Loaded existing vector DB from WindowsPath('C:/Users/ggn/1_py_scripts/RAG/RAG_data_analysis_hackathon_2025/persists/chroma_db_code3d_3').\n"
     ]
    }
   ],
   "source": [
    "new_persist = False # Set to True to create a new persist directory. Set to True while creating new db.\n",
    "folder_path = \"./stm_data_code_sample/3D_hyperspectral_data\"\n",
    "\n",
    "# Chunk size and overlap hyperparameters\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 200\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "persist_basename = \"chroma_db_code3d\"\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "temperature = 0  #Vary in the range 0-2.\n",
    "\n",
    "rag_chain = get_qa_rag_chain(\n",
    "    folder_path=folder_path,\n",
    "    persist_basename=persist_basename,\n",
    "    new_persist=new_persist,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a0a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history: []\n",
      "Human: \n",
      "Write code to plot the current map at a probe bias of 1.2 V for the cits_data.3ds\".\n",
      "- Import necessary libraries\n",
      "- The script should:\n",
      "  1) load ./stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds\n",
      "  2) extract the Z/height channel (or the most appropriate topography channel)\n",
      "  3) plot it with matplotlib imshow + colorbar\n",
      "  4) Do not save the image to file, just show it\n",
      "\n",
      "\n",
      "===== AI GENERATED PYTHON CODE (STRING) =====\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from CITS_Class import CITS_Analysis\n",
      "\n",
      "# Load the CITS data\n",
      "filepath = './stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds'\n",
      "data = CITS_Analysis(filepath)\n",
      "\n",
      "# Set the probe bias\n",
      "probe_bias = 1.2\n",
      "\n",
      "# Extract the current map at the specified probe bias\n",
      "i_2D, V_actual = data.current_map(probe_bias)\n",
      "print('Nearest Probed bias = ', V_actual)\n",
      "\n",
      "# Plot the current map\n",
      "plt.imshow(i_2D, aspect='auto', origin='lower')\n",
      "plt.colorbar()  # Add a colorbar to the plot\n",
      "plt.title(f'Current Map at Probe Bias of {probe_bias} V')\n",
      "plt.xlabel('X Position')\n",
      "plt.ylabel('Y Position')\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "chat_history = get_chat_history(session_id)\n",
    "print(\"chat_history:\", chat_history)\n",
    "task1 = \"\"\"\n",
    "Write code to plot the current map at a probe bias of 1.2 V for the cits_data.3ds\".\n",
    "- Import necessary libraries\n",
    "- The script should:\n",
    "  1) load ./stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds\n",
    "  2) extract the Z/height channel (or the most appropriate topography channel)\n",
    "  3) plot it with matplotlib imshow + colorbar\n",
    "  4) Do not save the image to file, just show it\n",
    "\"\"\"\n",
    "\n",
    "answer1 = rag_chain.invoke({\"input\": task1, \"chat_history\":chat_history})['answer']\n",
    "insert_application_logs(session_id, task1, answer1, \"gpt-5\")\n",
    "print(f\"Human: {task1}\")\n",
    "print(f\"\\n===== AI GENERATED PYTHON CODE (STRING) =====\\n{answer1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6f2b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history: [HumanMessage(content='\\nWrite code to plot the current map at a probe bias of 1.2 V for the cits_data.3ds\".\\n- Import necessary libraries\\n- The script should:\\n  1) load ./stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds\\n  2) extract the Z/height channel (or the most appropriate topography channel)\\n  3) plot it with matplotlib imshow + colorbar\\n  4) Do not save the image to file, just show it\\n', additional_kwargs={}, response_metadata={}), AIMessage(content=\"import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom CITS_Class import CITS_Analysis\\n\\n# Load the CITS data\\nfilepath = './stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds'\\ndata = CITS_Analysis(filepath)\\n\\n# Set the probe bias\\nprobe_bias = 1.2\\n\\n# Extract the current map at the specified probe bias\\ni_2D, V_actual = data.current_map(probe_bias)\\nprint('Nearest Probed bias = ', V_actual)\\n\\n# Plot the current map\\nplt.imshow(i_2D, aspect='auto', origin='lower')\\nplt.colorbar()  # Add a colorbar to the plot\\nplt.title(f'Current Map at Probe Bias of {probe_bias} V')\\nplt.xlabel('X Position')\\nplt.ylabel('Y Position')\\nplt.show()\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_history = get_chat_history(session_id)\n",
    "print(\"chat_history:\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af92f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: provide the code, but change the filepath to 'cits_data.3ds'\n",
      "\n",
      "===== AI GENERATED PYTHON CODE (STRING) =====\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from CITS_Class import CITS_Analysis\n",
      "\n",
      "# Load the CITS data\n",
      "filepath = 'cits_data.3ds'\n",
      "data = CITS_Analysis(filepath)\n",
      "\n",
      "# Set the probe bias\n",
      "probe_bias = 1.2\n",
      "\n",
      "# Extract the current map at the specified probe bias\n",
      "i_2D, V_actual = data.current_map(probe_bias)\n",
      "print('Nearest Probed bias = ', V_actual)\n",
      "\n",
      "# Plot the current map\n",
      "plt.imshow(i_2D, aspect='auto', origin='lower')\n",
      "plt.colorbar()  # Add a colorbar to the plot\n",
      "plt.title(f'Current Map at Probe Bias of {probe_bias} V')\n",
      "plt.xlabel('X Position')\n",
      "plt.ylabel('Y Position')\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task2 = \"provide the code, but change the filepath to 'cits_data.3ds'\"\n",
    "\n",
    "answer2 = rag_chain.invoke({\"input\": task2, \"chat_history\":chat_history})['answer']\n",
    "insert_application_logs(session_id, task2, answer2, \"gpt-4o-mini\")\n",
    "print(f\"Human: {task2}\")\n",
    "print(f\"\\n===== AI GENERATED PYTHON CODE (STRING) =====\\n\\n{answer2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd20e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Great. Now provide the didv_x map at the same probe_bias. Retain the same filepath\n",
      "\n",
      "===== AI GENERATED PYTHON CODE (STRING) =====\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from CITS_Class import CITS_Analysis\n",
      "\n",
      "# Load the CITS data\n",
      "filepath = './stm_data_code_sample/3D_hyperspectral_data/cits_data.3ds'\n",
      "data = CITS_Analysis(filepath)\n",
      "\n",
      "# Set the probe bias\n",
      "probe_bias = 1.2\n",
      "\n",
      "# Extract the didv_x map at the specified probe bias\n",
      "didv_x_2D, V_actual = data.didv_x_map(probe_bias)\n",
      "print('Nearest Probed bias = ', V_actual)\n",
      "\n",
      "# Plot the didv_x map\n",
      "plt.imshow(didv_x_2D, aspect='auto', origin='lower')\n",
      "plt.colorbar()  # Add a colorbar to the plot\n",
      "plt.title(f'didv_x Map at Probe Bias of {probe_bias} V')\n",
      "plt.xlabel('X Position')\n",
      "plt.ylabel('Y Position')\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task3 = \"Great. Now provide the didv_x map at the same probe_bias. Retain the same filepath\"\n",
    "answer3 = rag_chain.invoke({\"input\": task3, \"chat_history\":chat_history})['answer']\n",
    "insert_application_logs(session_id, task3, answer3, \"gpt-4o-mini\")\n",
    "print(f\"Human: {task3}\")\n",
    "print(f\"\\n===== AI GENERATED PYTHON CODE (STRING) =====\\n\\n{answer3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85669cef",
   "metadata": {},
   "source": [
    "## Test the code\n",
    "Write the code to \"main_rag_test.py\". Overwrites if already exists.\n",
    "\n",
    "Test it by running \"python main_rag_test.py\" on terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8720d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/ggn/1_py_scripts/RAG/RAG_data_analysis_hackathon_2025/main_rag_test.py')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_generated_code_to_file(\n",
    "    code_str=answer2,\n",
    "    filename=\"main_rag_test.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb266da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be9ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb25df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_env_kernel",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
